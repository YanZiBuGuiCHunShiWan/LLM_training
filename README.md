# 大模型训练

# 支持模型

- [Baichuan7B/13B](https://github.com/baichuan-inc/Baichuan-7B)
- [Moss16B](https://github.com/OpenLMLab/MOSS)
- [ChatGLM/ChatGLM2](https://github.com/THUDM/ChatGLM-6B)

# 支持训练方法

## SFT

- Full parameter tuning
- [LoRA](https://arxiv.org/abs/2106.09685)
- [QLoRA](https://arxiv.org/abs/2305.14314)

# 支持训练数据集

## SFT-Datasets

- [Belle_open_source_500k](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN/blob/main/Belle_open_source_0.5M.json)```单轮对话```
- [Firefly-train-1.1M](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)```单论对话```：23种常见的中文NLP任务的数据，并且构造了许多与中华文化相关的数据，如对联、作诗、文言文翻译、散文、金庸小说等。对于每个任务，由人工书写若干种指令模板，保证数据的高质量与丰富度，数据量为115万

- [Moss-003-sft-data](https://huggingface.co/datasets/YeungNLP/moss-003-sft-data)```多论对话```
- [Safety-prompts](https://github.com/thu-coai/Safety-Prompts)```单论对话```
- [WebNovel]()```单论对话```



# 