{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/baichuan7B/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/anaconda3/envs/baichuan7B/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.7/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /root/anaconda3/envs/baichuan7B/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/baichuan7B/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /root/anaconda3/envs/baichuan7B did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments, HfArgumentParser, set_seed\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"../../ptm/baichuan\"\n",
    "#model_path=\"/data/Baichuan-13B/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(model_path,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(model_path,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaiChuanForCausalLM(\n",
       "  (model): Model(\n",
       "    (embed_tokens): Embedding(64000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (self_attn): Attention(\n",
       "          (W_pack): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=64000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "<class 'transformers_modules.modeling_baichuan.BaiChuanForCausalLM'>\n",
      "['model']\n",
      "<class 'transformers_modules.modeling_baichuan.Model'>\n",
      "['model', 'embed_tokens']\n",
      "<class 'torch.nn.modules.sparse.Embedding'>\n",
      "['model', 'layers']\n",
      "<class 'torch.nn.modules.container.ModuleList'>\n",
      "['model', 'layers', '0']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '0', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '0', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '0', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '0', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '0', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '0', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '0', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '0', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '0', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '0', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '0', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '1']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '1', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '1', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '1', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '1', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '1', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '1', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '1', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '1', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '1', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '1', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '1', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '2']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '2', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '2', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '2', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '2', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '2', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '2', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '2', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '2', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '2', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '2', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '2', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '3']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '3', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '3', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '3', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '3', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '3', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '3', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '3', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '3', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '3', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '3', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '3', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '4']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '4', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '4', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '4', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '4', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '4', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '4', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '4', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '4', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '4', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '4', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '4', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '5']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '5', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '5', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '5', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '5', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '5', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '5', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '5', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '5', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '5', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '5', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '5', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '6']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '6', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '6', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '6', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '6', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '6', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '6', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '6', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '6', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '6', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '6', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '6', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '7']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '7', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '7', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '7', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '7', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '7', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '7', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '7', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '7', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '7', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '7', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '7', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '8']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '8', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '8', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '8', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '8', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '8', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '8', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '8', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '8', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '8', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '8', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '8', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '9']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '9', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '9', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '9', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '9', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '9', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '9', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '9', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '9', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '9', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '9', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '9', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '10']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '10', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '10', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '10', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '10', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '10', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '10', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '10', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '10', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '10', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '10', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '10', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '11']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '11', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '11', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '11', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '11', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '11', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '11', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '11', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '11', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '11', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '11', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '11', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '12']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '12', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '12', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '12', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '12', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '12', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '12', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '12', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '12', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '12', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '12', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '12', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '13']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '13', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '13', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '13', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '13', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '13', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '13', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '13', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '13', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '13', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '13', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '13', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '14']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '14', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '14', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '14', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '14', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '14', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '14', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '14', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '14', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '14', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '14', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '14', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '15']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '15', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '15', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '15', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '15', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '15', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '15', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '15', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '15', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '15', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '15', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '15', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '16']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '16', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '16', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '16', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '16', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '16', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '16', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '16', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '16', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '16', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '16', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '16', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '17']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '17', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '17', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '17', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '17', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '17', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '17', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '17', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '17', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '17', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '17', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '17', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '18']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '18', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '18', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '18', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '18', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '18', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '18', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '18', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '18', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '18', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '18', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '18', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '19']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '19', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '19', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '19', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '19', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '19', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '19', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '19', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '19', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '19', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '19', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '19', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '20']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '20', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '20', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '20', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '20', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '20', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '20', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '20', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '20', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '20', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '20', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '20', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '21']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '21', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '21', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '21', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '21', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '21', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '21', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '21', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '21', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '21', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '21', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '21', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '22']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '22', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '22', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '22', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '22', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '22', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '22', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '22', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '22', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '22', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '22', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '22', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '23']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '23', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '23', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '23', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '23', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '23', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '23', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '23', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '23', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '23', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '23', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '23', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '24']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '24', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '24', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '24', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '24', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '24', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '24', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '24', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '24', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '24', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '24', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '24', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '25']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '25', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '25', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '25', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '25', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '25', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '25', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '25', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '25', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '25', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '25', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '25', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '26']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '26', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '26', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '26', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '26', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '26', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '26', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '26', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '26', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '26', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '26', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '26', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '27']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '27', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '27', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '27', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '27', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '27', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '27', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '27', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '27', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '27', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '27', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '27', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '28']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '28', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '28', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '28', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '28', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '28', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '28', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '28', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '28', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '28', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '28', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '28', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '29']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '29', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '29', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '29', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '29', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '29', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '29', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '29', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '29', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '29', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '29', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '29', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '30']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '30', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '30', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '30', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '30', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '30', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '30', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '30', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '30', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '30', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '30', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '30', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '31']\n",
      "<class 'transformers_modules.modeling_baichuan.DecoderLayer'>\n",
      "['model', 'layers', '31', 'self_attn']\n",
      "<class 'transformers_modules.modeling_baichuan.Attention'>\n",
      "['model', 'layers', '31', 'self_attn', 'W_pack']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '31', 'self_attn', 'o_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '31', 'self_attn', 'rotary_emb']\n",
      "<class 'transformers_modules.modeling_baichuan.RotaryEmbedding'>\n",
      "['model', 'layers', '31', 'mlp']\n",
      "<class 'transformers_modules.modeling_baichuan.MLP'>\n",
      "['model', 'layers', '31', 'mlp', 'gate_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '31', 'mlp', 'down_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '31', 'mlp', 'up_proj']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['model', 'layers', '31', 'mlp', 'act_fn']\n",
      "<class 'transformers.activations.SiLUActivation'>\n",
      "['model', 'layers', '31', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'layers', '31', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['model', 'norm']\n",
      "<class 'transformers_modules.modeling_baichuan.RMSNorm'>\n",
      "['lm_head']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "for names,parameter in model.named_modules():\n",
    "    print(names.split(\".\"))\n",
    "    print(type(parameter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(model_path,trust_remote_code=True,quantization_config=BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaiChuanForCausalLM(\n",
       "  (model): Model(\n",
       "    (embed_tokens): Embedding(64000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (self_attn): Attention(\n",
       "          (W_pack): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=64000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_modules(model):\n",
    "    cls=bnb.nn.Linear4bit\n",
    "    lora_target_modules=set()\n",
    "    for names,module in model.named_modules():\n",
    "        if isinstance(module,cls):\n",
    "            names=names.split(\".\")\n",
    "            lora_target_modules.add(names[0] if len(names)==1 else names[-1])\n",
    "            \n",
    "    if 'lm_head' in lora_target_modules: # needed for 16-bit\n",
    "        lora_target_modules.remove('lm_head')\n",
    "    return list(lora_target_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=find_all_linear_modules(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=LoraConfig(\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=l\n",
    "    )\n",
    "model=get_peft_model(model,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27,893,760 || all params: 6,988,231,680 || trainable%: 0.3991533377439484\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "j=0\n",
    "for names,parameter in model.named_modules():\n",
    "    if parameter.requires_grad_==True:\n",
    "        print(\"Yes\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27,893,760 || all params: 6,988,231,680 || trainable%: 0.3991533377439484\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "a=model.print_trainable_parameters()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"/data/sftmoss16B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "model=AutoModelForCausalLM.from_pretrained(model_path,trust_remote_code=True,device_map={\"\":1}, quantization_config=BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "            ),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaiChuanForCausalLM(\n",
       "  (model): Model(\n",
       "    (embed_tokens): Embedding(64000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (self_attn): Attention(\n",
       "          (W_pack): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=64000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "['transformer']\n",
      "['transformer', 'wte']\n",
      "['transformer', 'drop']\n",
      "['transformer', 'h']\n",
      "['transformer', 'h', '0']\n",
      "['transformer', 'h', '0', 'ln_1']\n",
      "['transformer', 'h', '0', 'attn']\n",
      "['transformer', 'h', '0', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '0', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '0', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '0', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '0', 'mlp']\n",
      "['transformer', 'h', '0', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '0', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '0', 'mlp', 'act']\n",
      "['transformer', 'h', '0', 'mlp', 'dropout']\n",
      "['transformer', 'h', '1']\n",
      "['transformer', 'h', '1', 'ln_1']\n",
      "['transformer', 'h', '1', 'attn']\n",
      "['transformer', 'h', '1', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '1', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '1', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '1', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '1', 'mlp']\n",
      "['transformer', 'h', '1', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '1', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '1', 'mlp', 'act']\n",
      "['transformer', 'h', '1', 'mlp', 'dropout']\n",
      "['transformer', 'h', '2']\n",
      "['transformer', 'h', '2', 'ln_1']\n",
      "['transformer', 'h', '2', 'attn']\n",
      "['transformer', 'h', '2', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '2', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '2', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '2', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '2', 'mlp']\n",
      "['transformer', 'h', '2', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '2', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '2', 'mlp', 'act']\n",
      "['transformer', 'h', '2', 'mlp', 'dropout']\n",
      "['transformer', 'h', '3']\n",
      "['transformer', 'h', '3', 'ln_1']\n",
      "['transformer', 'h', '3', 'attn']\n",
      "['transformer', 'h', '3', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '3', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '3', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '3', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '3', 'mlp']\n",
      "['transformer', 'h', '3', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '3', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '3', 'mlp', 'act']\n",
      "['transformer', 'h', '3', 'mlp', 'dropout']\n",
      "['transformer', 'h', '4']\n",
      "['transformer', 'h', '4', 'ln_1']\n",
      "['transformer', 'h', '4', 'attn']\n",
      "['transformer', 'h', '4', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '4', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '4', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '4', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '4', 'mlp']\n",
      "['transformer', 'h', '4', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '4', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '4', 'mlp', 'act']\n",
      "['transformer', 'h', '4', 'mlp', 'dropout']\n",
      "['transformer', 'h', '5']\n",
      "['transformer', 'h', '5', 'ln_1']\n",
      "['transformer', 'h', '5', 'attn']\n",
      "['transformer', 'h', '5', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '5', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '5', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '5', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '5', 'mlp']\n",
      "['transformer', 'h', '5', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '5', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '5', 'mlp', 'act']\n",
      "['transformer', 'h', '5', 'mlp', 'dropout']\n",
      "['transformer', 'h', '6']\n",
      "['transformer', 'h', '6', 'ln_1']\n",
      "['transformer', 'h', '6', 'attn']\n",
      "['transformer', 'h', '6', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '6', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '6', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '6', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '6', 'mlp']\n",
      "['transformer', 'h', '6', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '6', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '6', 'mlp', 'act']\n",
      "['transformer', 'h', '6', 'mlp', 'dropout']\n",
      "['transformer', 'h', '7']\n",
      "['transformer', 'h', '7', 'ln_1']\n",
      "['transformer', 'h', '7', 'attn']\n",
      "['transformer', 'h', '7', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '7', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '7', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '7', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '7', 'mlp']\n",
      "['transformer', 'h', '7', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '7', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '7', 'mlp', 'act']\n",
      "['transformer', 'h', '7', 'mlp', 'dropout']\n",
      "['transformer', 'h', '8']\n",
      "['transformer', 'h', '8', 'ln_1']\n",
      "['transformer', 'h', '8', 'attn']\n",
      "['transformer', 'h', '8', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '8', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '8', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '8', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '8', 'mlp']\n",
      "['transformer', 'h', '8', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '8', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '8', 'mlp', 'act']\n",
      "['transformer', 'h', '8', 'mlp', 'dropout']\n",
      "['transformer', 'h', '9']\n",
      "['transformer', 'h', '9', 'ln_1']\n",
      "['transformer', 'h', '9', 'attn']\n",
      "['transformer', 'h', '9', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '9', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '9', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '9', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '9', 'mlp']\n",
      "['transformer', 'h', '9', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '9', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '9', 'mlp', 'act']\n",
      "['transformer', 'h', '9', 'mlp', 'dropout']\n",
      "['transformer', 'h', '10']\n",
      "['transformer', 'h', '10', 'ln_1']\n",
      "['transformer', 'h', '10', 'attn']\n",
      "['transformer', 'h', '10', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '10', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '10', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '10', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '10', 'mlp']\n",
      "['transformer', 'h', '10', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '10', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '10', 'mlp', 'act']\n",
      "['transformer', 'h', '10', 'mlp', 'dropout']\n",
      "['transformer', 'h', '11']\n",
      "['transformer', 'h', '11', 'ln_1']\n",
      "['transformer', 'h', '11', 'attn']\n",
      "['transformer', 'h', '11', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '11', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '11', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '11', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '11', 'mlp']\n",
      "['transformer', 'h', '11', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '11', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '11', 'mlp', 'act']\n",
      "['transformer', 'h', '11', 'mlp', 'dropout']\n",
      "['transformer', 'h', '12']\n",
      "['transformer', 'h', '12', 'ln_1']\n",
      "['transformer', 'h', '12', 'attn']\n",
      "['transformer', 'h', '12', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '12', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '12', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '12', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '12', 'mlp']\n",
      "['transformer', 'h', '12', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '12', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '12', 'mlp', 'act']\n",
      "['transformer', 'h', '12', 'mlp', 'dropout']\n",
      "['transformer', 'h', '13']\n",
      "['transformer', 'h', '13', 'ln_1']\n",
      "['transformer', 'h', '13', 'attn']\n",
      "['transformer', 'h', '13', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '13', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '13', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '13', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '13', 'mlp']\n",
      "['transformer', 'h', '13', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '13', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '13', 'mlp', 'act']\n",
      "['transformer', 'h', '13', 'mlp', 'dropout']\n",
      "['transformer', 'h', '14']\n",
      "['transformer', 'h', '14', 'ln_1']\n",
      "['transformer', 'h', '14', 'attn']\n",
      "['transformer', 'h', '14', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '14', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '14', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '14', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '14', 'mlp']\n",
      "['transformer', 'h', '14', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '14', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '14', 'mlp', 'act']\n",
      "['transformer', 'h', '14', 'mlp', 'dropout']\n",
      "['transformer', 'h', '15']\n",
      "['transformer', 'h', '15', 'ln_1']\n",
      "['transformer', 'h', '15', 'attn']\n",
      "['transformer', 'h', '15', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '15', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '15', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '15', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '15', 'mlp']\n",
      "['transformer', 'h', '15', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '15', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '15', 'mlp', 'act']\n",
      "['transformer', 'h', '15', 'mlp', 'dropout']\n",
      "['transformer', 'h', '16']\n",
      "['transformer', 'h', '16', 'ln_1']\n",
      "['transformer', 'h', '16', 'attn']\n",
      "['transformer', 'h', '16', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '16', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '16', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '16', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '16', 'mlp']\n",
      "['transformer', 'h', '16', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '16', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '16', 'mlp', 'act']\n",
      "['transformer', 'h', '16', 'mlp', 'dropout']\n",
      "['transformer', 'h', '17']\n",
      "['transformer', 'h', '17', 'ln_1']\n",
      "['transformer', 'h', '17', 'attn']\n",
      "['transformer', 'h', '17', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '17', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '17', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '17', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '17', 'mlp']\n",
      "['transformer', 'h', '17', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '17', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '17', 'mlp', 'act']\n",
      "['transformer', 'h', '17', 'mlp', 'dropout']\n",
      "['transformer', 'h', '18']\n",
      "['transformer', 'h', '18', 'ln_1']\n",
      "['transformer', 'h', '18', 'attn']\n",
      "['transformer', 'h', '18', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '18', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '18', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '18', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '18', 'mlp']\n",
      "['transformer', 'h', '18', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '18', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '18', 'mlp', 'act']\n",
      "['transformer', 'h', '18', 'mlp', 'dropout']\n",
      "['transformer', 'h', '19']\n",
      "['transformer', 'h', '19', 'ln_1']\n",
      "['transformer', 'h', '19', 'attn']\n",
      "['transformer', 'h', '19', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '19', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '19', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '19', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '19', 'mlp']\n",
      "['transformer', 'h', '19', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '19', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '19', 'mlp', 'act']\n",
      "['transformer', 'h', '19', 'mlp', 'dropout']\n",
      "['transformer', 'h', '20']\n",
      "['transformer', 'h', '20', 'ln_1']\n",
      "['transformer', 'h', '20', 'attn']\n",
      "['transformer', 'h', '20', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '20', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '20', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '20', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '20', 'mlp']\n",
      "['transformer', 'h', '20', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '20', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '20', 'mlp', 'act']\n",
      "['transformer', 'h', '20', 'mlp', 'dropout']\n",
      "['transformer', 'h', '21']\n",
      "['transformer', 'h', '21', 'ln_1']\n",
      "['transformer', 'h', '21', 'attn']\n",
      "['transformer', 'h', '21', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '21', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '21', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '21', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '21', 'mlp']\n",
      "['transformer', 'h', '21', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '21', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '21', 'mlp', 'act']\n",
      "['transformer', 'h', '21', 'mlp', 'dropout']\n",
      "['transformer', 'h', '22']\n",
      "['transformer', 'h', '22', 'ln_1']\n",
      "['transformer', 'h', '22', 'attn']\n",
      "['transformer', 'h', '22', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '22', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '22', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '22', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '22', 'mlp']\n",
      "['transformer', 'h', '22', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '22', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '22', 'mlp', 'act']\n",
      "['transformer', 'h', '22', 'mlp', 'dropout']\n",
      "['transformer', 'h', '23']\n",
      "['transformer', 'h', '23', 'ln_1']\n",
      "['transformer', 'h', '23', 'attn']\n",
      "['transformer', 'h', '23', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '23', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '23', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '23', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '23', 'mlp']\n",
      "['transformer', 'h', '23', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '23', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '23', 'mlp', 'act']\n",
      "['transformer', 'h', '23', 'mlp', 'dropout']\n",
      "['transformer', 'h', '24']\n",
      "['transformer', 'h', '24', 'ln_1']\n",
      "['transformer', 'h', '24', 'attn']\n",
      "['transformer', 'h', '24', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '24', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '24', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '24', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '24', 'mlp']\n",
      "['transformer', 'h', '24', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '24', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '24', 'mlp', 'act']\n",
      "['transformer', 'h', '24', 'mlp', 'dropout']\n",
      "['transformer', 'h', '25']\n",
      "['transformer', 'h', '25', 'ln_1']\n",
      "['transformer', 'h', '25', 'attn']\n",
      "['transformer', 'h', '25', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '25', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '25', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '25', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '25', 'mlp']\n",
      "['transformer', 'h', '25', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '25', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '25', 'mlp', 'act']\n",
      "['transformer', 'h', '25', 'mlp', 'dropout']\n",
      "['transformer', 'h', '26']\n",
      "['transformer', 'h', '26', 'ln_1']\n",
      "['transformer', 'h', '26', 'attn']\n",
      "['transformer', 'h', '26', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '26', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '26', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '26', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '26', 'mlp']\n",
      "['transformer', 'h', '26', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '26', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '26', 'mlp', 'act']\n",
      "['transformer', 'h', '26', 'mlp', 'dropout']\n",
      "['transformer', 'h', '27']\n",
      "['transformer', 'h', '27', 'ln_1']\n",
      "['transformer', 'h', '27', 'attn']\n",
      "['transformer', 'h', '27', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '27', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '27', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '27', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '27', 'mlp']\n",
      "['transformer', 'h', '27', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '27', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '27', 'mlp', 'act']\n",
      "['transformer', 'h', '27', 'mlp', 'dropout']\n",
      "['transformer', 'h', '28']\n",
      "['transformer', 'h', '28', 'ln_1']\n",
      "['transformer', 'h', '28', 'attn']\n",
      "['transformer', 'h', '28', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '28', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '28', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '28', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '28', 'mlp']\n",
      "['transformer', 'h', '28', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '28', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '28', 'mlp', 'act']\n",
      "['transformer', 'h', '28', 'mlp', 'dropout']\n",
      "['transformer', 'h', '29']\n",
      "['transformer', 'h', '29', 'ln_1']\n",
      "['transformer', 'h', '29', 'attn']\n",
      "['transformer', 'h', '29', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '29', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '29', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '29', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '29', 'mlp']\n",
      "['transformer', 'h', '29', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '29', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '29', 'mlp', 'act']\n",
      "['transformer', 'h', '29', 'mlp', 'dropout']\n",
      "['transformer', 'h', '30']\n",
      "['transformer', 'h', '30', 'ln_1']\n",
      "['transformer', 'h', '30', 'attn']\n",
      "['transformer', 'h', '30', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '30', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '30', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '30', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '30', 'mlp']\n",
      "['transformer', 'h', '30', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '30', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '30', 'mlp', 'act']\n",
      "['transformer', 'h', '30', 'mlp', 'dropout']\n",
      "['transformer', 'h', '31']\n",
      "['transformer', 'h', '31', 'ln_1']\n",
      "['transformer', 'h', '31', 'attn']\n",
      "['transformer', 'h', '31', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '31', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '31', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '31', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '31', 'mlp']\n",
      "['transformer', 'h', '31', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '31', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '31', 'mlp', 'act']\n",
      "['transformer', 'h', '31', 'mlp', 'dropout']\n",
      "['transformer', 'h', '32']\n",
      "['transformer', 'h', '32', 'ln_1']\n",
      "['transformer', 'h', '32', 'attn']\n",
      "['transformer', 'h', '32', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '32', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '32', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '32', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '32', 'mlp']\n",
      "['transformer', 'h', '32', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '32', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '32', 'mlp', 'act']\n",
      "['transformer', 'h', '32', 'mlp', 'dropout']\n",
      "['transformer', 'h', '33']\n",
      "['transformer', 'h', '33', 'ln_1']\n",
      "['transformer', 'h', '33', 'attn']\n",
      "['transformer', 'h', '33', 'attn', 'attn_dropout']\n",
      "['transformer', 'h', '33', 'attn', 'resid_dropout']\n",
      "['transformer', 'h', '33', 'attn', 'qkv_proj']\n",
      "Yes\n",
      "['transformer', 'h', '33', 'attn', 'out_proj']\n",
      "Yes\n",
      "['transformer', 'h', '33', 'mlp']\n",
      "['transformer', 'h', '33', 'mlp', 'fc_in']\n",
      "Yes\n",
      "['transformer', 'h', '33', 'mlp', 'fc_out']\n",
      "Yes\n",
      "['transformer', 'h', '33', 'mlp', 'act']\n",
      "['transformer', 'h', '33', 'mlp', 'dropout']\n",
      "['transformer', 'ln_f']\n",
      "['lm_head']\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "j=0\n",
    "for names,parameter in model.named_modules():\n",
    "    print(names.split(\".\"))\n",
    "    i+=1\n",
    "    if isinstance(parameter,bnb.nn.Linear4bit):\n",
    "        print(\"Yes\")\n",
    "        j+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"../../ptm/chatglm2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(model_path,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "model=AutoModel.from_pretrained(model_path,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "<class 'transformers_modules.modeling_chatglm.ChatGLMForConditionalGeneration'>\n",
      "['transformer']\n",
      "<class 'transformers_modules.modeling_chatglm.ChatGLMModel'>\n",
      "['transformer', 'embedding']\n",
      "<class 'transformers_modules.modeling_chatglm.Embedding'>\n",
      "['transformer', 'embedding', 'word_embeddings']\n",
      "<class 'torch.nn.modules.sparse.Embedding'>\n",
      "['transformer', 'rotary_pos_emb']\n",
      "<class 'transformers_modules.modeling_chatglm.RotaryEmbedding'>\n",
      "['transformer', 'encoder']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMTransformer'>\n",
      "['transformer', 'encoder', 'layers']\n",
      "<class 'torch.nn.modules.container.ModuleList'>\n",
      "['transformer', 'encoder', 'layers', '0']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '0', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '0', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '0', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '0', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '0', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '0', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '0', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '0', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '0', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '0', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '1']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '1', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '1', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '1', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '1', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '1', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '1', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '1', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '1', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '1', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '1', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '2']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '2', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '2', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '2', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '2', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '2', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '2', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '2', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '2', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '2', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '2', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '3']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '3', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '3', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '3', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '3', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '3', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '3', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '3', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '3', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '3', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '3', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '4']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '4', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '4', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '4', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '4', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '4', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '4', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '4', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '4', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '4', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '4', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '5']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '5', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '5', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '5', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '5', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '5', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '5', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '5', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '5', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '5', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '5', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '6']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '6', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '6', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '6', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '6', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '6', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '6', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '6', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '6', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '6', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '6', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '7']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '7', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '7', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '7', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '7', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '7', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '7', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '7', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '7', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '7', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '7', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '8']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '8', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '8', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '8', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '8', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '8', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '8', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '8', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '8', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '8', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '8', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '9']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '9', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '9', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '9', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '9', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '9', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '9', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '9', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '9', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '9', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '9', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '10']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '10', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '10', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '10', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '10', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '10', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '10', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '10', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '10', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '10', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '10', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '11']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '11', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '11', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '11', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '11', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '11', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '11', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '11', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '11', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '11', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '11', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '12']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '12', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '12', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '12', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '12', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '12', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '12', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '12', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '12', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '12', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '12', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '13']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '13', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '13', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '13', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '13', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '13', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '13', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '13', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '13', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '13', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '13', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '14']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '14', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '14', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '14', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '14', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '14', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '14', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '14', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '14', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '14', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '14', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '15']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '15', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '15', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '15', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '15', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '15', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '15', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '15', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '15', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '15', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '15', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '16']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '16', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '16', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '16', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '16', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '16', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '16', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '16', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '16', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '16', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '16', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '17']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '17', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '17', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '17', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '17', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '17', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '17', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '17', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '17', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '17', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '17', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '18']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '18', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '18', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '18', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '18', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '18', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '18', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '18', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '18', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '18', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '18', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '19']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '19', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '19', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '19', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '19', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '19', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '19', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '19', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '19', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '19', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '19', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '20']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '20', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '20', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '20', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '20', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '20', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '20', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '20', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '20', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '20', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '20', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '21']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '21', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '21', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '21', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '21', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '21', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '21', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '21', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '21', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '21', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '21', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '22']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '22', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '22', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '22', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '22', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '22', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '22', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '22', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '22', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '22', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '22', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '23']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '23', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '23', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '23', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '23', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '23', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '23', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '23', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '23', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '23', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '23', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '24']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '24', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '24', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '24', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '24', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '24', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '24', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '24', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '24', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '24', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '24', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '25']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '25', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '25', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '25', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '25', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '25', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '25', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '25', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '25', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '25', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '25', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '26']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '26', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '26', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '26', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '26', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '26', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '26', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '26', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '26', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '26', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '26', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '27']\n",
      "<class 'transformers_modules.modeling_chatglm.GLMBlock'>\n",
      "['transformer', 'encoder', 'layers', '27', 'input_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '27', 'self_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.SelfAttention'>\n",
      "['transformer', 'encoder', 'layers', '27', 'self_attention', 'query_key_value']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '27', 'self_attention', 'core_attention']\n",
      "<class 'transformers_modules.modeling_chatglm.CoreAttention'>\n",
      "['transformer', 'encoder', 'layers', '27', 'self_attention', 'core_attention', 'attention_dropout']\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "['transformer', 'encoder', 'layers', '27', 'self_attention', 'dense']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '27', 'post_attention_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'encoder', 'layers', '27', 'mlp']\n",
      "<class 'transformers_modules.modeling_chatglm.MLP'>\n",
      "['transformer', 'encoder', 'layers', '27', 'mlp', 'dense_h_to_4h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'layers', '27', 'mlp', 'dense_4h_to_h']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "['transformer', 'encoder', 'final_layernorm']\n",
      "<class 'transformers_modules.modeling_chatglm.RMSNorm'>\n",
      "['transformer', 'output_layer']\n",
      "<class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "for names,parameter in model.named_modules():\n",
    "    print(names.split(\".\"))\n",
    "    print(type(parameter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"../data/Safetyprompts/typical_safety_scenarios.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-510eeaa93a98ff80/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    }
   ],
   "source": [
    "data=load_dataset(\"json\",data_files=data_path,field='Crimes_And_Illegal_Activities',split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['type', 'response', 'prompt'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-f4835834a62e263f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 5833.52it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1111.37it/s]\n",
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-f4835834a62e263f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "data1=load_dataset(\"json\",data_files=data_path,field='Mental_Health',split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['type', 'response', 'prompt'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataall=data.add_item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/baichuan7B/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data():\n",
    "    for obj in [data,data1]:\n",
    "        for j in obj:\n",
    "            yield j\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /root/.cache/huggingface/datasets/generator/default-3e1c76b2ef6964fd/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /root/.cache/huggingface/datasets/generator/default-3e1c76b2ef6964fd/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "ds=Dataset.from_generator(gen_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['type', 'response', 'prompt'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path=\"../data/Moss-sft/moss-003-sft-no-tools.jsonl\"\n",
    "data_path=\"../data/Firefly/firefly-train-1.1M.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conversation_id', 'meta_instruction', 'num_turns', 'chat', 'category'])\n"
     ]
    }
   ],
   "source": [
    "a=set()\n",
    "with jsonlines.open(data_path,\"r\") as f:\n",
    "    for line in f:\n",
    "        print(line.keys())\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{dict}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(dp):\n",
    "    with jsonlines.open(dp,\"r\") as f:\n",
    "        for line in f:\n",
    "            yield line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /root/.cache/huggingface/datasets/generator/default-94aae615ee10a7e6/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /root/.cache/huggingface/datasets/generator/default-94aae615ee10a7e6/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "ds=Dataset.from_generator(gen,gen_kwargs={\"dp\":data_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1649399"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kind', 'input', 'target']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baichuan7B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
